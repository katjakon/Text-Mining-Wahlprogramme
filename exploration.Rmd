---
title: "Untersuchungen der Wahlprogramme zur Bundestagswahl"
author: "Anina Klaus, Katja Konermann und Niklas Stepczynski"
output: html_document
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Erste Schritte

Das Korpus wurde mithilfe der Bibliothek `quanteda` untersucht. Außerdem wird die Bibliothek `readtext` benötigt, um die Textdateien einlesen zu können.

```{r library, message=FALSE}
library(quanteda)
library(readtext)
```

Um das Korpus einlesen zu können, muss sich der Ordner `Korpus-Dateien` mit den Wahlprogrammen im aktuellen Arbeitsverzeichnis befinden. Die Namen der Dateien sind nach dem folgenden Muster aufgebaut: `<Wahl>-<Partei>-<Jahr>`. Das Wahlprogramm der CDU zur Bundestagswahl von 2017 ist zum Beispiel in der Datei `Bundestagswahl-CDU-2017` gespeichert. Mithilfe von `readtext` können aus den Dateinamen so Document Level Variables erstellt werden, die das Filtern nach Parteien und Jahren vereinfachen. <br>
Zusätzlich wird die Spalte `year` in numerische Werte konvertiert, um auch Vergleiche wie etwa Wahlprogramme vor 2009 zu ermöglichen.

```{r Einlesen}
# Read in files and set document level variables
programs <- readtext("Korpus-Dateien", 
                     docvarsfrom = "filenames",
                     docvarnames = c("type", "party", "year"),
                     dvsep = "-",
                     encoding="utf-8") %>% corpus()

# Convert characters in year column to integers
docvars(programs, field="year") <- as.integer(docvars(programs, field="year"))
```
Mit der `summary`- Funktion kann ein erster Überblick über das Korpus gegeben werden. Dabei ist auffällig, dass die Wahlprogramme sich in ihrer Länge mitunter sehr unterscheiden: Beispielsweise ist das Wahlrprogramm der AFD von 2017 nur etwa 450 Token lang, während die meisten anderen Wahlprogramme über 5000 Token lang sind. 
```{r Zusammenfassung}
summary(programs)
```

Um das Korpus zu tokenisieren wird die `tokens`-Funktion von `quanteda` genutzt, wobei Satzzeichen und Stoppwörter entfernt werden. Die genutzten Stoppwörter sind in `quanteda` enthalten und umfassen folgende Token:

```{r stoppwoerter}
stopwords("german")
```
Mit der `dfm`-Funktion von `quanteda` wird zudem eine Document-Feature-Matrix erstellt werden.

```{r tokens_dfm}
# Create tokens object for whole corpus
program_toks <- tokens(programs,remove_punct = TRUE) %>% tokens_remove(stopwords("german"), padding = TRUE )
# Create dfm for corpus
program_dfm <- dfm(program_toks)
```


## Bag of Words

Mithilfe der Funktion `topfeatures` lassen sich aus einer `DFM` die n häufigsten Token extrahieren. Diese Token können auch mit `textplot_wordcloud` as Wortwolke dargestellt werden. 

```{r wortwolke_whole}
set.seed(2021)
# word cloud for whole corpus
textplot_wordcloud(program_dfm, max_words = 100, min_size = 1.7, color = "darkslategrey")
```

Die Funktion `texplot_wordcloud` besitzt einen optionalen Parameter `comparison`, der es erlaubt, vergleichende Wortwolken zu erstellen. Dabei werden für eine Gruppe die Token ausgewählt, die im Vergleich zu den anderen Gruppen häufig vorkommen. Genaueres ist [hier in der Dokumentation](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/comparison.cloud) nachlesbar.
So wurden vergleichende Wortwolken für die verschiedenen Parteien und die Jahre erstellt.

```{r wortwolke_years}
years <- dfm(program_toks, groups = "year")
textplot_wordcloud(years, max_words = 100, min_size = 0.5, comparison=TRUE, color=c("darkolivegreen", "cadetblue4", "deeppink3", "darkorange", "darkred"))
```

```{r wortwolke_party}
parties <- dfm(program_toks, groups = "party")
textplot_wordcloud(parties, max_words = 100, min_size = 0.5, comparison=TRUE, color=c("blue3", "darkgreen", "black", "red", "deeppink", "darkred", "brown2"))
```

## Keywords in Context

Mit der `kwic`-Funktion können Token ausgegeben werden, die zusammen mit einem oder mehreren Schlüsselwörtern in einem bestimmten Fenster auftreten. Für das Schlüsselwort `deutsch*` - Der Asterix steht dabei für beliebige Folgezeichen - sieht der Rückgabewert etwa folgendermaßen aus:

```{r kwic_deutsch}
head(kwic(program_toks,"deutsch*", window=2))
```

Auch aus dem Rückgabewert der `kwic`- Funktion lassen sich Wortwolken erstellen, die zeigen, mit welchen Token die Schlüsselwörter am häufigsten gemeinsam auftreten. So wird hier ein kleines Wörterbuch für das Thema (`umwelt*`, `klima*`, `nachhalt*`) Klima und Europa (`eu*`, `europ*`) erstellt, Kontextwörter in einem Fenster von 10 Token mithilfe der `kwic`-Funktion ermittelt und diese dann durch `texplot_wordcloud` veranschaulicht.

```{r kwic_klima}
# climate change dictionary
climate_dict <- c("klima*", "umwelt*", "nachhalt*")

# Word cloud for climate change dictionary
klima <- kwic(program_toks,climate_dict, window=10) 
textplot_wordcloud(klima %>% corpus() %>% dfm(), max_words = 100, color= "chartreuse4")
```

```{r kwic_eu}
# eu dictionary
eu_dict <- c("eu*", "europ*")

# Word cloud for climate change dictionary
eu <- kwic(program_toks,eu_dict, window=10) 
textplot_wordcloud(eu%>% corpus() %>% dfm(), max_words = 100)
```

### Lexcial Dispersion


## Kollokationen

## TF-IDF

## Topic Modelling
