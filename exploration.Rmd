---
title: "Untersuchungen der Wahlprogramme zur Bundestagswahl"
author: "Anina Klaus, Katja Konermann und Niklas Stepczynski"
output: html_document
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Erste Schritte

Das Korpus wurde mithilfe der Bibliothek `quanteda` untersucht. Außerdem wird die Bibliothek `readtext` benötigt, um die Textdateien einlesen zu können.

```{r library, message=FALSE}
library(quanteda)
library(readtext)
```

Um das Korpus einlesen zu können, muss sich der Ordner `Korpus-Dateien` mit den Wahlprogrammen im aktuellen Arbeitsverzeichnis befinden. Die Namen der Dateien sind nach dem folgenden Muster aufgebaut: `<Wahl>-<Partei>-<Jahr>`. Das Wahlprogramm der CDU zur Bundestagswahl von 2017 ist zum Beispiel in der Datei `Bundestagswahl-CDU-2017` gespeichert. Mithilfe von `readtext` können aus den Dateinamen so Document Level Variables erstellt werden, die das Filtern nach Parteien und Jahren vereinfachen. <br>
Zusätzlich wird die Spalte `year` in numerische Werte konvertiert, um auch Vergleiche wie etwa Wahlprogramme vor 2009 zu ermöglichen.

```{r Einlesen}
# Read in files and set document level variables
programs <- readtext("Korpus-Dateien", 
                     docvarsfrom = "filenames",
                     docvarnames = c("type", "party", "year"),
                     dvsep = "-",
                     encoding="utf-8") %>% corpus()

# Convert characters in year column to integers
docvars(programs, field="year") <- as.integer(docvars(programs, field="year"))
```
Mit der `summary`- Funktion kann ein erster Überblick über das Korpus gegeben werden. Dabei ist auffällig, dass die Wahlprogramme sich in ihrer Länge mitunter sehr unterscheiden: Beispielsweise ist das Wahlrprogramm der AFD von 2017 nur etwa 450 Token lang, während die meisten anderen Wahlprogramme über 5000 Token lang sind. 
```{r Zusammenfassung}
summary(programs)
```

Um das Korpus zu tokenisieren wird die `tokens`-Funktion von `quanteda` genutzt, wobei Satzzeichen und Stoppwörter entfernt werden. Die genutzten Stoppwörter sind in `quanteda` enthalten und umfassen folgende Token:

```{r stoppwoerter}
stopwords("german")
```
Mit der `dfm`-Funktion von `quanteda` wird zudem eine Document-Feature-Matrix erstellt werden.

```{r tokens_dfm}
# Create tokens object for whole corpus
program_toks <- tokens(programs,remove_punct = TRUE) %>% tokens_remove(stopwords("german"), padding = TRUE )
# Create dfm for corpus
program_dfm <- dfm(program_toks)
```


## Bag of Words

Mithilfe der Funktion `topfeatures` lassen sich aus einer `DFM` die n häufigsten Token extrahieren. Diese Token können auch mit `textplot_wordcloud` as Wortwolke dargestellt werden. 

```{r wortwolke_whole}
set.seed(2021)
# word cloud for whole corpus
textplot_wordcloud(program_dfm, max_words = 100, min_size = 1.7, color = "darkslategrey")
```

## Keywords in Context

### Lexcial Dispersion

## Kollokationen

## TF-IDF

## Topic Modelling
